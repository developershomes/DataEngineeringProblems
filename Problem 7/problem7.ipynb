{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4328d022-1f8d-442f-921e-d16693058a4c",
   "metadata": {},
   "source": [
    "Here, we will solve problems two ways\n",
    "1. First using PySpark function \n",
    "2. Second using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4647c5-df06-4d53-b4b4-66677cc54ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Load all the required library and also Start Spark Session\n",
    "# Load all the required library\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fdceb9-20df-4588-8820-672d48778b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/15 10:13:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"problem7\").getOrCreate()\n",
    "sqlContext = SparkSession(spark)\n",
    "#Dont Show warning only error\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ec58af-280e-4eef-a95e-308df1bcbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Load CSV file into DataFrame\n",
    "transactiondf = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6604a74-b1f5-49e5-a593-f35ca2417030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check Schema of DataFrame\n",
    "transactiondf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ba5185-8682-4b49-88b8-9391cd0c2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "transactiondf = transactiondf.withColumn(\"created_at\",col(\"created_at\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59514ce5-8584-4b67-9cff-934e9287f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- created_at: date (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactiondf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47481142-ee32-401e-a481-03b3dd5b80ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+----------+-------+\n",
      "| id|user_id|   item|created_at|revenue|\n",
      "+---+-------+-------+----------+-------+\n",
      "|  1|    109|   milk|2020-03-03|    123|\n",
      "|  2|    139|biscuit|2020-03-18|    421|\n",
      "|  3|    120|   milk|2020-03-18|    176|\n",
      "|  4|    108| banana|2020-03-18|    862|\n",
      "|  5|    130|   milk|2020-03-28|    333|\n",
      "|  6|    103|  bread|2020-03-29|    862|\n",
      "|  7|    122| banana|2020-03-07|    952|\n",
      "|  8|    125|  bread|2020-03-13|    317|\n",
      "|  9|    139|  bread|2020-03-30|    929|\n",
      "| 10|    141| banana|2020-03-17|    812|\n",
      "| 11|    116|  bread|2020-03-31|    226|\n",
      "| 12|    128|  bread|2020-03-04|    112|\n",
      "| 13|    146|biscuit|2020-03-04|    362|\n",
      "| 14|    119| banana|2020-03-28|    127|\n",
      "| 15|    142|  bread|2020-03-09|    503|\n",
      "| 16|    122|  bread|2020-03-06|    593|\n",
      "| 17|    128|biscuit|2020-03-24|    160|\n",
      "| 18|    112| banana|2020-03-24|    262|\n",
      "| 19|    149| banana|2020-03-29|    382|\n",
      "| 20|    100| banana|2020-03-18|    599|\n",
      "+---+-------+-------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check sample Data \n",
    "transactiondf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c28f990b-7e88-4c88-bd36-ca17a83544c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are solving Same problem using Spark SQL \n",
    "# Creating Temp Table or HIVE table\n",
    "transactiondf.createOrReplaceTempView(\"tmpTransaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a48a300-9f44-4321-a138-942e6f1daf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|    100|\n",
      "|    103|\n",
      "|    105|\n",
      "|    109|\n",
      "|    110|\n",
      "|    111|\n",
      "|    112|\n",
      "|    114|\n",
      "|    117|\n",
      "|    120|\n",
      "|    122|\n",
      "|    128|\n",
      "|    129|\n",
      "|    130|\n",
      "|    131|\n",
      "|    133|\n",
      "|    141|\n",
      "|    143|\n",
      "|    150|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we have SQL Table and we can write SQL Query on top of that \n",
    "# For example by Select on table \n",
    "sqlContext.sql(\"SELECT DISTINCT(a1.user_id) \\\n",
    "                FROM tmpTransaction a1 \\\n",
    "                JOIN tmpTransaction a2 ON a1.user_id=a2.user_id \\\n",
    "                AND a1.id <> a2.id \\\n",
    "                AND DATEDIFF(a2.created_at,a1.created_at) BETWEEN 0 AND 7 \\\n",
    "                ORDER BY a1.user_id;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55eb16a-fb5c-42b6-9f7c-feb1ff9c2945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
